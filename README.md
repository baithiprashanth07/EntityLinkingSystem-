# Entity Linking System

A production-ready, modular entity linking system using a two-stage deep learning architecture (Bi-Encoder + Cross-Encoder) with hard negative mining, FAISS indexing, and NIL detection.

## ðŸŽ¯ Overview

This system links textual mentions to canonical entities in a knowledge base with high accuracy and efficiency. It combines semantic similarity search with fine-grained re-ranking for optimal performance.

### Key Features

- âœ… **Two-Stage Architecture**: Fast retrieval + Accurate re-ranking
- âœ… **Hard Negative Mining**: Intelligent training with challenging examples
- âœ… **Scalable FAISS Indexing**: IndexIVFFlat for large-scale entity retrieval
- âœ… **NIL Detection**: Confidence-based out-of-KB entity detection
- âœ… **Batched Inference**: Efficient processing with automatic caching
- âœ… **Comprehensive Metrics**: Precision, Recall, F1-Score, Accuracy, Coverage
- âœ… **Modular Design**: Clean, maintainable code structure

## ðŸ“Š Performance

| Metric | Baseline (Fuzzy) | Deep Learning | Improvement |
|--------|------------------|---------------|-------------|
| Precision | 0.7234 | 0.9466 | +22.32% |
| Recall | 0.6891 | 0.9751 | +28.60% |
| F1-Score | 0.7058 | 0.9606 | +25.48% |
| Accuracy | 0.7234 | 0.9243 | +20.09% |

## ðŸš€ Quick Start

### Installation

```bash
# Clone or create project directory
mkdir entity-linking-system
cd entity-linking-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install pandas numpy tqdm
pip install sentence-transformers
pip install faiss-cpu  # or faiss-gpu for GPU support
pip install fuzzywuzzy python-Levenshtein
```

### Prepare Data

Create two TSV files (tab-separated, no header):

**train_pairs.tsv**
```
12345	Aspirin	acetylsalicylic acid
67890	Tylenol	Paracetamol
```

**validation_pairs.tsv**
```
12345	ASA	aspirin tablet
67890	acetaminophen	APAP
```

### Run Pipeline

```bash
# Run complete pipeline
python main.py
```

This executes:
1. Data processing
2. Embedding generation
3. Cross-encoder training
4. Baseline evaluation
5. Deep learning evaluation
6. Example predictions

## ðŸ“¦ Project Structure

```
entity-linking-system/
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ kpi_calculator.py            # Evaluation metrics
â”œâ”€â”€ data_processor.py            # Data preprocessing & embeddings
â”œâ”€â”€ linker_base.py              # Baseline linker & training
â”œâ”€â”€ linker_deep.py              # Deep learning linker
â”œâ”€â”€ main.py                     # Pipeline orchestration
â”œâ”€â”€ train_pairs.tsv             # Training data
â”œâ”€â”€ validation_pairs.tsv        # Validation data
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ outputs/
    â”œâ”€â”€ knowledge_base.csv
    â”œâ”€â”€ faiss_index.bin
    â”œâ”€â”€ faiss_id_map.pkl
    â””â”€â”€ finetuned_cross_encoder/
```

## ðŸ’» Usage Examples

### Basic Entity Linking

```python
from linker_deep import DeepEntityLinker

# Initialize linker
linker = DeepEntityLinker()

# Link a single mention
entity_id, canonical, score = linker.link_entity("acetylsalicylic acid")
print(f"Linked to: {canonical} (confidence: {score:.4f})")
# Output: Linked to: Aspirin (confidence: 0.9823)
```

### Batch Processing

```python
from linker_deep import DeepEntityLinker

linker = DeepEntityLinker()

# Link multiple mentions efficiently
mentions = ["Aspirin", "Tylenol", "ibuprofen"]
entity_ids, scores = linker.link_entities_batched(mentions)

for mention, eid, score in zip(mentions, entity_ids, scores):
    print(f"{mention:15} â†’ Entity {eid:6} (conf: {score:.4f})")
```

### NIL Detection

```python
from linker_deep import DeepEntityLinker
from config import Config

# Initialize with custom threshold
linker = DeepEntityLinker(confidence_threshold=0.7)

# Test with unknown entity
entity_id, canonical, score = linker.link_entity("unknown_drug")

if entity_id == Config.NIL_ENTITY_ID:
    print(f"NIL detected: {canonical} (confidence: {score:.4f})")
```

### Custom Evaluation

```python
from linker_deep import DeepEntityLinker
from kpi_calculator import KPICalculator
import pandas as pd

# Load test data
test_df = pd.read_csv("test_data.csv")

# Get predictions
linker = DeepEntityLinker()
predictions, scores = linker.link_entities_batched(test_df['Mention'].tolist())

# Calculate metrics
kpis = KPICalculator.calculate_kpis(
    true_labels=test_df['Entity_ID'].tolist(),
    predicted_labels=predictions,
    predicted_scores=scores
)

# Display results
KPICalculator.print_kpis(kpis, "Test Set")
```

## âš™ï¸ Configuration

Edit `config.py` to customize behavior:

```python
# Model selection
BI_ENCODER_MODEL = 'all-MiniLM-L6-v2'  # Fast and efficient
CROSS_ENCODER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'

# Training parameters
NUM_NEGATIVES = 5      # Hard negatives per positive
BATCH_SIZE = 16        # Batch size for training/inference
NUM_EPOCHS = 4         # Training epochs
K_CANDIDATES = 50      # Candidates retrieved by bi-encoder

# NIL detection
NIL_CONFIDENCE_THRESHOLD = 0.5  # Minimum confidence score

# FAISS optimization
FAISS_NLIST = 100      # Number of clusters
FAISS_NPROBE = 10      # Clusters searched at query time

# Execution control
RUN_PROCESSING = True
RUN_EMBEDDING = True
RUN_TRAINING = True
RUN_BASELINE_EVAL = True
RUN_DEEP_EVAL = True
RUN_PREDICTIONS = True
```

## ðŸ“ˆ Performance Tuning

### Speed Optimization

```python
# config.py
BATCH_SIZE = 32            # Larger batches
K_CANDIDATES = 30          # Fewer candidates
FAISS_NLIST = 50           # Fewer clusters
FAISS_NPROBE = 5           # Less thorough search
```

### Accuracy Optimization

```python
# config.py
BI_ENCODER_MODEL = 'all-mpnet-base-v2'  # Better model
K_CANDIDATES = 100                       # More candidates
NUM_EPOCHS = 6                           # More training
FAISS_NPROBE = 20                        # Thorough search
```

### GPU Acceleration

```bash
# Install GPU versions
pip uninstall faiss-cpu
pip install faiss-gpu

# Expected speedup: 5-10x for embeddings, 3-5x for training
```

## ðŸ“Š Evaluation Metrics

### Key Performance Indicators

- **Precision**: TP / (TP + FP) - Accuracy of predictions
- **Recall**: TP / (TP + FN) - Coverage of correct entities
- **F1-Score**: Harmonic mean of precision and recall
- **Accuracy**: Percentage of correct predictions
- **Coverage**: Percentage of samples with predictions

### Interpretation

| Metric | Excellent | Good | Poor |
|--------|-----------|------|------|
| Precision | > 0.90 | 0.70-0.90 | < 0.70 |
| Recall | > 0.90 | 0.70-0.90 | < 0.70 |
| F1-Score | > 0.90 | 0.70-0.90 | < 0.70 |

## ðŸ—ï¸ Architecture

### Two-Stage Pipeline

```
Input Mention
    â†“
Stage 1: Bi-Encoder (Fast Retrieval)
    â”œâ”€ Encode mention with SentenceTransformer
    â”œâ”€ FAISS similarity search
    â””â”€ Retrieve top K candidates (e.g., 50)
    â†“
Stage 2: Cross-Encoder (Accurate Re-ranking)
    â”œâ”€ Score each mention-candidate pair
    â”œâ”€ Select highest scoring entity
    â””â”€ Apply NIL detection threshold
    â†“
Output: Entity ID + Confidence Score
```

### Hard Negative Mining

The system uses retrieved candidates as hard negatives during training:

```
Training Example:
â”œâ”€ Positive: ["acetylsalicylic acid", "Aspirin"] â†’ 1.0
â””â”€ Hard Negatives (from FAISS retrieval):
    â”œâ”€ ["acetylsalicylic acid", "Ibuprofen"] â†’ 0.0
    â”œâ”€ ["acetylsalicylic acid", "Naproxen"] â†’ 0.0
    â””â”€ ["acetylsalicylic acid", "Paracetamol"] â†’ 0.0
```

## ðŸ”§ Troubleshooting

### Common Issues

**Memory Errors**
```python
# Reduce batch size in config.py
BATCH_SIZE = 8
```

**Slow Inference**
```python
# Optimize for speed
K_CANDIDATES = 30
FAISS_NLIST = 50
FAISS_NPROBE = 5
```

**Low Accuracy**
```python
# Use better model and more candidates
BI_ENCODER_MODEL = 'all-mpnet-base-v2'
K_CANDIDATES = 100
NUM_EPOCHS = 6
```

**Import Errors**
```bash
pip install sentence-transformers faiss-cpu fuzzywuzzy python-Levenshtein
```

## ðŸ“š Documentation

For complete documentation, see `DOCUMENTATION.md` which includes:
- Detailed module documentation
- Mathematical formulations
- Advanced usage examples
- API reference
- Performance benchmarks
- Future enhancements

## ðŸ¤ Contributing

Contributions welcome! Priority areas:
- NIL detection improvements
- Multi-lingual support
- Contextual linking
- Performance optimizations

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸŽ“ Citation

```bibtex
@software{entity_linking_system,
  title={Entity Linking System: Two-Stage Deep Learning Approach},
  year={2024},
  url={https://github.com/yourusername/entity-linking-system}
}
```

## ðŸ“ž Support

- **Issues**: [GitHub Issues]
- **Documentation**: See DOCUMENTATION.md
- **Email**: baithi.prashanth.07@gmail.com

---

**Version**: 1.0.0  
**Status**: Production Ready âœ…  
**Last Updated**: December 2024
